{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loadArticles\n",
    "import loadStockInfo\n",
    "import textProcessing\n",
    "\n",
    "import en_core_web_lg\n",
    "\n",
    "import requests\n",
    "import urllib.parse\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.pipeline import EntityRecognizer\n",
    "from spacy.tokens import Doc\n",
    "from spacy.pipeline import EntityRuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sklearn packages\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load stock info using meta and historical data\n",
    "stockInfo = loadStockInfo.loadStockInfo('../../data/stock-market-dataset/stocks/', '../../data/stock-market-dataset/symbols_valid_meta.csv', DEBUG=True)\n",
    "#stock_df = stockInfo.loadStockDf()\n",
    "stock_meta_df = pd.read_csv('../../data/stock-market-dataset/symbols_valid_meta.csv', index_col=0)\n",
    "#stock_meta_df = stock_meta_df[['Symbol', 'Security Name']]\n",
    "# stock_meta_df['Tag'] = 'ORG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_df = stock_meta_df[stock_meta_df['ETF'] == 'N']['NASDAQ Symbol']\n",
    "# symbol_df = symbol_df[symbol_df != 'FB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_by_ticker_dict = {'ticker': [], 'title': [], 'sentiment': [], 'date': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          A\n",
       "1         AA\n",
       "2       AACG\n",
       "3        AAL\n",
       "4       AAMC\n",
       "        ... \n",
       "5879     ZUO\n",
       "5880     ZVO\n",
       "5881    ZYME\n",
       "5882    ZYNE\n",
       "5883    ZYXI\n",
       "Name: NASDAQ Symbol, Length: 5884, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# symbol_df[symbol_df == 'UTX#']\n",
    "# symbol_df.iloc[54] = 'CARR'\n",
    "symbol_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies').content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html(html_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_500 = tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     MMM\n",
       "1     ABT\n",
       "2    ABBV\n",
       "3    ABMD\n",
       "4     ACN\n",
       "Name: Symbol, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list of symbols to build news from focus on S&P 500 Index Stocks\n",
    "SP_500['Symbol'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# req = requests.get(url=\"https://stocknewsapi.com/api/v1?tickers=MMM&items=50&token=udkw09gzwcqp3zffbhsrcgiqegnytwyda7hlxazy&page=12\")\n",
    "# response = req.json()\n",
    "# print(response['error'] is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# #max 50 per request\n",
    "# token_api = \"udkw09gzwcqp3zffbhsrcgiqegnytwyda7hlxazy\"\n",
    "# #f\"https://stocknewsapi.com/api/v1?tickers={ticker}&items=10000&token={token_api}\"\n",
    "# not_counted = ['BRK-B', 'BF-B']\n",
    "# for ticker in not_counted:\n",
    "#     #number of pages\n",
    "#     number_of_pages = 1\n",
    "#     num = number_of_pages\n",
    "#     while num <= number_of_pages:\n",
    "#         # for ticker in symbol_df:\n",
    "#         stock_news_url = str(\"https://stocknewsapi.com/api/v1?tickers={}&items=50&token={}&page={}\").format(ticker, token_api, num)\n",
    "\n",
    "#         req = requests.get(url=stock_news_url)\n",
    "#         response = req.json()\n",
    "        \n",
    "#         if 'error' in response:\n",
    "#             break\n",
    "#         else:\n",
    "#             number_of_pages = int(response['total_pages'])\n",
    "            \n",
    "#         if response['data'] is None:\n",
    "#             continue\n",
    "#         else:\n",
    "#             for i in response['data']:\n",
    "#                 news_by_ticker_dict['ticker'].append(ticker)\n",
    "#                 news_by_ticker_dict['title'].append(i['title'])\n",
    "#                 news_by_ticker_dict['sentiment'].append(i['sentiment'])\n",
    "#                 news_by_ticker_dict['date'].append(i['date'])\n",
    "#         print(f\"{ticker} - {num}/{number_of_pages}\", end='\\n')\n",
    "#         num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_sp_500 = pd.DataFrame(news_by_ticker_dict)\n",
    "# news_sp_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_sp_500['ticker'] = np.where(news_sp_500['ticker'] == 'BRK-B', 'BRK.B', news_sp_500['ticker'])\n",
    "# news_sp_500['ticker'] = np.where(news_sp_500['ticker'] == 'BF-B', 'BF.B', news_sp_500['ticker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_sp_500.sort_values(by='ticker', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_sp_500.to_csv('/Users/jjackson/Brainstation/Capstone Project/news_sp_500.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Stock Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Feature Price Change of Day (Positive/Negative/Neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZYXIW\r"
     ]
    }
   ],
   "source": [
    "stock_df = stockInfo.loadStockDf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24197442, 9)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = stock_df.query('Date > \"2018-01-30\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2986292, 9)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df.columns = ['date', 'open', 'high', 'low', 'close', 'adj_close', 'volume', 'symbol','name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>symbol</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4578</th>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>73.769997</td>\n",
       "      <td>74.389999</td>\n",
       "      <td>73.239998</td>\n",
       "      <td>73.430000</td>\n",
       "      <td>71.956703</td>\n",
       "      <td>2032800.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies, Inc. Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4579</th>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>73.180000</td>\n",
       "      <td>73.779999</td>\n",
       "      <td>72.510002</td>\n",
       "      <td>72.830002</td>\n",
       "      <td>71.368736</td>\n",
       "      <td>2008200.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies, Inc. Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4580</th>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>72.320000</td>\n",
       "      <td>72.760002</td>\n",
       "      <td>71.220001</td>\n",
       "      <td>71.250000</td>\n",
       "      <td>69.820442</td>\n",
       "      <td>1955700.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies, Inc. Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4581</th>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>70.860001</td>\n",
       "      <td>71.480003</td>\n",
       "      <td>68.180000</td>\n",
       "      <td>68.220001</td>\n",
       "      <td>66.851227</td>\n",
       "      <td>2860700.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies, Inc. Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4582</th>\n",
       "      <td>2018-02-06</td>\n",
       "      <td>66.959999</td>\n",
       "      <td>68.830002</td>\n",
       "      <td>66.129997</td>\n",
       "      <td>68.449997</td>\n",
       "      <td>67.076614</td>\n",
       "      <td>4121200.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies, Inc. Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24197437</th>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>10.230000</td>\n",
       "      <td>11.430000</td>\n",
       "      <td>10.230000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>189500.0</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>Zynex, Inc. - Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24197438</th>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>10.980000</td>\n",
       "      <td>10.060000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>145000.0</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>Zynex, Inc. - Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24197439</th>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>10.160000</td>\n",
       "      <td>11.060000</td>\n",
       "      <td>10.160000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>162300.0</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>Zynex, Inc. - Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24197440</th>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>10.680000</td>\n",
       "      <td>11.140000</td>\n",
       "      <td>10.590000</td>\n",
       "      <td>11.070000</td>\n",
       "      <td>11.070000</td>\n",
       "      <td>280400.0</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>Zynex, Inc. - Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24197441</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>11.160000</td>\n",
       "      <td>11.160000</td>\n",
       "      <td>10.510000</td>\n",
       "      <td>10.920000</td>\n",
       "      <td>10.920000</td>\n",
       "      <td>315900.0</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>Zynex, Inc. - Common Stock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2986292 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                date       open       high        low      close  adj_close  \\\n",
       "4578      2018-01-31  73.769997  74.389999  73.239998  73.430000  71.956703   \n",
       "4579      2018-02-01  73.180000  73.779999  72.510002  72.830002  71.368736   \n",
       "4580      2018-02-02  72.320000  72.760002  71.220001  71.250000  69.820442   \n",
       "4581      2018-02-05  70.860001  71.480003  68.180000  68.220001  66.851227   \n",
       "4582      2018-02-06  66.959999  68.830002  66.129997  68.449997  67.076614   \n",
       "...              ...        ...        ...        ...        ...        ...   \n",
       "24197437  2020-03-26  10.230000  11.430000  10.230000  11.100000  11.100000   \n",
       "24197438  2020-03-27  10.700000  10.980000  10.060000  10.300000  10.300000   \n",
       "24197439  2020-03-30  10.160000  11.060000  10.160000  10.800000  10.800000   \n",
       "24197440  2020-03-31  10.680000  11.140000  10.590000  11.070000  11.070000   \n",
       "24197441  2020-04-01  11.160000  11.160000  10.510000  10.920000  10.920000   \n",
       "\n",
       "             volume symbol                                     name  \n",
       "4578      2032800.0      A  Agilent Technologies, Inc. Common Stock  \n",
       "4579      2008200.0      A  Agilent Technologies, Inc. Common Stock  \n",
       "4580      1955700.0      A  Agilent Technologies, Inc. Common Stock  \n",
       "4581      2860700.0      A  Agilent Technologies, Inc. Common Stock  \n",
       "4582      4121200.0      A  Agilent Technologies, Inc. Common Stock  \n",
       "...             ...    ...                                      ...  \n",
       "24197437   189500.0   ZYXI               Zynex, Inc. - Common Stock  \n",
       "24197438   145000.0   ZYXI               Zynex, Inc. - Common Stock  \n",
       "24197439   162300.0   ZYXI               Zynex, Inc. - Common Stock  \n",
       "24197440   280400.0   ZYXI               Zynex, Inc. - Common Stock  \n",
       "24197441   315900.0   ZYXI               Zynex, Inc. - Common Stock  \n",
       "\n",
       "[2986292 rows x 9 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_by_ticker_df = pd.read_csv(\"../../data/news/final_news_sent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>title</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FB</td>\n",
       "      <td>By Refusing to Act, Is Zuckerberg Hurting Face...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Sat, 06 Jun 2020 12:23:00 -0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FB</td>\n",
       "      <td>Zuckerberg: Facebook will review policies afte...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Sat, 06 Jun 2020 00:24:13 -0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FB</td>\n",
       "      <td>Facebook, Instagram Take Down Trump George Flo...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Fri, 05 Jun 2020 23:50:21 -0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FB</td>\n",
       "      <td>After Employee Backlash Over Trump Posts, Zuck...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Fri, 05 Jun 2020 22:36:29 -0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FB</td>\n",
       "      <td>Facebook's Zuckerberg promises a review of con...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Fri, 05 Jun 2020 20:23:01 -0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291208</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>Will Boeing go lower? #AskHalftime</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Wed, 12 Feb 2020 13:48:26 -0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291209</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>Zoetis (ZTS) to Report Q4 Earnings: What's in ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Mon, 10 Feb 2020 09:25:00 -0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291210</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>Zoetis (ZTS) Earnings Expected to Grow: Should...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Thu, 06 Feb 2020 12:31:08 -0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291211</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>Zoetis Inc. (ZTS) CEO Kristin Peck on Q4 2019 ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Thu, 13 Feb 2020 15:30:46 -0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291212</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>Zoetis CEO: Banking on Man’s Best Friend? - Ma...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Mon, 18 Feb 2019 19:00:00 -0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>291213 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ticker                                              title sentiment  \\\n",
       "0          FB  By Refusing to Act, Is Zuckerberg Hurting Face...   Neutral   \n",
       "1          FB  Zuckerberg: Facebook will review policies afte...   Neutral   \n",
       "2          FB  Facebook, Instagram Take Down Trump George Flo...   Neutral   \n",
       "3          FB  After Employee Backlash Over Trump Posts, Zuck...   Neutral   \n",
       "4          FB  Facebook's Zuckerberg promises a review of con...   Neutral   \n",
       "...       ...                                                ...       ...   \n",
       "291208    ZTS                 Will Boeing go lower? #AskHalftime   Neutral   \n",
       "291209    ZTS  Zoetis (ZTS) to Report Q4 Earnings: What's in ...   Neutral   \n",
       "291210    ZTS  Zoetis (ZTS) Earnings Expected to Grow: Should...  Positive   \n",
       "291211    ZTS  Zoetis Inc. (ZTS) CEO Kristin Peck on Q4 2019 ...   Neutral   \n",
       "291212    ZTS  Zoetis CEO: Banking on Man’s Best Friend? - Ma...   Neutral   \n",
       "\n",
       "                                   date  \n",
       "0       Sat, 06 Jun 2020 12:23:00 -0400  \n",
       "1       Sat, 06 Jun 2020 00:24:13 -0400  \n",
       "2       Fri, 05 Jun 2020 23:50:21 -0400  \n",
       "3       Fri, 05 Jun 2020 22:36:29 -0400  \n",
       "4       Fri, 05 Jun 2020 20:23:01 -0400  \n",
       "...                                 ...  \n",
       "291208  Wed, 12 Feb 2020 13:48:26 -0500  \n",
       "291209  Mon, 10 Feb 2020 09:25:00 -0500  \n",
       "291210  Thu, 06 Feb 2020 12:31:08 -0500  \n",
       "291211  Thu, 13 Feb 2020 15:30:46 -0500  \n",
       "291212  Mon, 18 Feb 2019 19:00:00 -0500  \n",
       "\n",
       "[291213 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_by_ticker_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive    133725\n",
       "Neutral     108401\n",
       "Negative     49087\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_by_ticker_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.bar(news_by_ticker_df['sentiment'].value_counts().index, news_by_ticker_df['sentiment'].value_counts().values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291213, 4)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_by_ticker_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get entities from article headline\n",
    "nlp = en_core_web_lg.load()\n",
    "ruler = EntityRuler(nlp)\n",
    "\n",
    "#creating pattersn for entity ruler\n",
    "temp_df1 = stock_meta_df[['Tag', 'Symbol']]\n",
    "temp_df1.columns = ['label', 'pattern']\n",
    "temp_df2 = stock_meta_df[['Tag', 'Name']]\n",
    "temp_df2.columns = ['label', 'pattern']\n",
    "\n",
    "temp_df = pd.concat([temp_df1, temp_df2])\n",
    "\n",
    "#set patterns\n",
    "patterns = temp_df.to_dict('records')\n",
    "#add patterns to ruler\n",
    "ruler.add_patterns(patterns)\n",
    "#add ruler to nlp\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "tp = textProcessing.textProcessing(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format date to connect stock info later\n",
    "news_by_ticker_df['date'] = pd.to_datetime(pd.to_datetime(news_by_ticker_df['date'], utc=True).dt.strftime(\"%m/%d/%y\"))\n",
    "\n",
    "#now that we have news_df which has the stock symbol we can derive sentiment and compare with historical price changes\n",
    "#create mutliple sets of transformed text, train test split data X is Headline y is sentiment\n",
    "#convert sentiment\n",
    "# 0 -> Positive\n",
    "# 1 -> Negative\n",
    "# 2 -> Neutral\n",
    "news_by_ticker_df['sentiment'] = np.where(news_by_ticker_df['sentiment'] == 'Positive', 0, news_by_ticker_df['sentiment'])\n",
    "news_by_ticker_df['sentiment'] = np.where(news_by_ticker_df['sentiment'] == 'Negative', 1, news_by_ticker_df['sentiment'])\n",
    "news_by_ticker_df['sentiment'] = np.where(news_by_ticker_df['sentiment'] == 'Neutral', 2, news_by_ticker_df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticker               object\n",
      "title                object\n",
      "sentiment             int32\n",
      "date         datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "news_by_ticker_df = news_by_ticker_df.astype({'sentiment': 'int32'})\n",
    "print(news_by_ticker_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up GridSearchCV for best params of DT and Logistic Regression\n",
    "lr_parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "dt_parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set X and y\n",
    "X = news_by_ticker_df['title']\n",
    "y = news_by_ticker_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since the size of the df is (134611, 4) I feel we can have a validation set\n",
    "X_remainder, X_test, y_remainder, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# X_train, X_validation, y_train, y_validation = train_test_split(X_remainder, y_remainder, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75381,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32307,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text processing\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " def tokenizeNLTK(sentence):\n",
    "    try:\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        ENGLISH_STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "        for punctuation_mark in string.punctuation:\n",
    "            # Remove punctuation and set to lower case\n",
    "            sentence = sentence.replace(punctuation_mark,'').lower()\n",
    "\n",
    "        # split sentence into words\n",
    "        listofwords = sentence.split(' ')\n",
    "        listofstemmed_words = []\n",
    "\n",
    "\n",
    "        # Remove stopwords and any tokens that are just empty strings\n",
    "        for word in listofwords:\n",
    "            if (not word in ENGLISH_STOP_WORDS) and (word!=''):\n",
    "                # Stem words\n",
    "                stemmed_word = stemmer.stem(word)\n",
    "                listofstemmed_words.append(stemmed_word)\n",
    "\n",
    "        return listofstemmed_words\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Something went wrong in tokenizeNLTK: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeSpacy(sentence):\n",
    "    try:\n",
    "        for punctuation_mark in string.punctuation:\n",
    "            # Remove punctuation and set to lower case\n",
    "            sentence = sentence.replace(punctuation_mark,'').lower()\n",
    "\n",
    "        doc = nlp(sentence)\n",
    "\n",
    "        listofwords = list()\n",
    "        for token in doc:\n",
    "            if not token.is_stop:\n",
    "                if token.is_alpha:\n",
    "                    listofwords.append(token.lemma_.strip().lower())\n",
    "\n",
    "        return listofwords\n",
    "    except Exception as e:\n",
    "        print(f\"Something went wrong in tokenizeSpacy: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_line = \"\\n=======================================================\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer on X_train, X_validation using Spacy\n",
    "countVectorizerSpacy = CountVectorizer(tokenizer=tokenizeSpacy, min_df=10, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectorizerSpacy.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform with Spacy CountVectorizer\n",
    "X_train_countSpacy = countVectorizerSpacy.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countSpacy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation_countSpacy = countVectorizerSpacy.transform(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert back to dataframes\n",
    "X_train_countSpacy = pd.DataFrame(X_train_countSpacy.toarray(), columns=countVectorizerSpacy.get_feature_names())\n",
    "X_validation_countSpacy = pd.DataFrame(X_validation_countSpacy.toarray(), columns=countVectorizerSpacy.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train shape: {X_train_countSpacy.shape} - X_validation shape: {X_validation_countSpacy.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try different models see accuracy\n",
    "#for CountVectorize we try LogisticRegression & Decision Trees\n",
    "#train and run on validation set to hypertune Spacy\n",
    "logit_spacy = LogisticRegression(max_iter=1000)\n",
    "logit_spacy.fit(X_train_countSpacy, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted\n",
    "y_pred_spacy = logit_spacy.predict(X_validation_countSpacy)\n",
    "report_spacy = classification_report(y_validation, y_pred_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation Spacy Confusion Matrix:\", print_line, confusion_matrix(y_validation, y_pred_spacy))\n",
    "print(\"\\nValidation Spacy Data Classification Report:\", print_line, report_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_range = [.000001, .00001, .0001, .001, .01,.1, 1, 10, 100]\n",
    "train_scores = list()\n",
    "validation_scores = list()\n",
    "for index, c in enumerate(C_range):\n",
    "    print(index, end='\\r')\n",
    "    logit_test = LogisticRegression(C=c, max_iter=1000)\n",
    "    logit_test.fit(X_train_countSpacy, y_train)\n",
    "    validation_scores.append(logit_test.score(X_validation_countSpacy, y_validation))\n",
    "    train_scores.append(logit_test.score(X_train_countSpacy, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot validation and train scores\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.plot(range(6), validation_scores, label=\"Validation\")\n",
    "plt.plot(range(6), train_scores, label=\"Train\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Position for C_range\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_c = C_range[validation_scores.index(np.max(validation_scores))]\n",
    "best_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count vectorizer\n",
    "countVectorizerSpacy.fit(X_remainder)\n",
    "X_remainder_countSpacy = countVectorizerSpacy.transform(X_remainder)\n",
    "X_test_countSpacy = countVectorizerSpacy.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_remainder_countSpacy = pd.DataFrame(X_remainder_countSpacy.toarray(), columns=countVectorizerSpacy.get_feature_names())\n",
    "X_test_countSpacy = pd.DataFrame(X_test_countSpacy.toarray(), columns=countVectorizerSpacy.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_remainder_countSpacy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_countSpacy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run on test set\n",
    "logit_spacy = LogisticRegression(C=best_c, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_spacy.fit(X_remainder_countSpacy, y_remainder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted\n",
    "y_pred_test_spacy = logit_spacy.predict(X_test_countSpacy)\n",
    "report_test_spacy = classification_report(y_test, y_pred_test_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Spacy Confusion Matrix:\", print_line, confusion_matrix(y_test, y_pred_test_spacy))\n",
    "print(\"\\Test Spacy Data Classification Report:\", print_line, report_test_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jjackson/opt/anaconda3/envs/BrainStationCapstone/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(min_df=10, ngram_range=(1, 3),\n",
       "                tokenizer=<function tokenizeNLTK at 0x7fcf62f1ae18>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CountVectorizer on X_train, X_validation using NLTK\n",
    "countVectorizerNLTK = CountVectorizer(tokenizer=tokenizeNLTK, min_df=10, ngram_range=(1,3))\n",
    "countVectorizerNLTK.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform with NLTK CountVectorizer\n",
    "X_train_countNLTK = countVectorizerNLTK.transform(X_train)\n",
    "X_validation_countNLTK = countVectorizerNLTK.transform(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'toarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-808559146fee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#convert back to dataframes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train_countNLTK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_countNLTK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcountVectorizerNLTK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_validation_countNLTK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_validation_countNLTK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcountVectorizerNLTK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/BrainStationCapstone/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'toarray'"
     ]
    }
   ],
   "source": [
    "#convert back to dataframes\n",
    "X_train_countNLTK = pd.DataFrame(X_train_countNLTK.toarray(), columns=countVectorizerNLTK.get_feature_names())\n",
    "X_validation_countNLTK = pd.DataFrame(X_validation_countNLTK.toarray(), columns=countVectorizerNLTK.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (75381, 14866) - X_validation shape: (32307, 14866)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train shape: {X_train_countNLTK.shape} - X_validation shape: {X_validation_countNLTK.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train and run on validation set to hypertune NLTK\n",
    "logit_NLTK = LogisticRegression(max_iter=1000)\n",
    "logit_NLTK.fit(X_train_countNLTK, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_NLTK = logit_NLTK.predict(X_validation_countNLTK)\n",
    "report_NLTK = classification_report(y_validation, y_pred_NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Validation Confusion Matrix: \n",
      "=======================================================\n",
      " [[12762   218  1307]\n",
      " [  369  3915   484]\n",
      " [ 1445   382 11425]]\n",
      "\n",
      "NLTK Validation Data Classification Report: \n",
      "=======================================================\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.88     14287\n",
      "           1       0.87      0.82      0.84      4768\n",
      "           2       0.86      0.86      0.86     13252\n",
      "\n",
      "    accuracy                           0.87     32307\n",
      "   macro avg       0.87      0.86      0.86     32307\n",
      "weighted avg       0.87      0.87      0.87     32307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"NLTK Validation Confusion Matrix:\", print_line, confusion_matrix(y_validation, y_pred_NLTK))\n",
    "print(\"\\nNLTK Validation Data Classification Report:\", print_line, report_NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count vectorizer\n",
    "countVectorizerNLTK.fit(X_remainder)\n",
    "X_remainder_countNLTK = countVectorizerNLTK.transform(X_remainder)\n",
    "X_test_countNLTK = countVectorizerNLTK.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_remainder_countNLTK = pd.DataFrame(X_remainder_countNLTK.toarray(), columns=countVectorizerNLTK.get_feature_names())\n",
    "X_test_countNLTK = pd.DataFrame(X_test_countNLTK.toarray(), columns=countVectorizerNLTK.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run on test set\n",
    "logit_NLTK = LogisticRegression(C=0.01, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, max_iter=1000)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_NLTK.fit(X_remainder_countNLTK, y_remainder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted\n",
    "y_pred_test_nltk = logit_NLTK.predict(X_test_countNLTK)\n",
    "report_test_nltk = classification_report(y_test, y_pred_test_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Spacy Confusion Matrix: \n",
      "=======================================================\n",
      " [[10494    98  1245]\n",
      " [  641  2643   746]\n",
      " [ 1378   146  9532]]\n",
      "\n",
      "Test Spacy Data Classification Report: \n",
      "=======================================================\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86     11837\n",
      "           1       0.92      0.66      0.76      4030\n",
      "           2       0.83      0.86      0.84     11056\n",
      "\n",
      "    accuracy                           0.84     26923\n",
      "   macro avg       0.86      0.80      0.82     26923\n",
      "weighted avg       0.85      0.84      0.84     26923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Spacy Confusion Matrix:\", print_line, confusion_matrix(y_test, y_pred_test_nltk))\n",
    "print(\"\\nTest Spacy Data Classification Report:\", print_line, report_test_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Spacy\n",
    "tfidf_Spacy = TfidfVectorizer(min_df=10, tokenizer=tokenizeSpacy, ngram_range = (1,3))\n",
    "tfidf_Spacy.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform with NLTK CountVectorizer\n",
    "X_train_tfidfSpacy = tfidf_Spacy.transform(X_train)\n",
    "X_validation_tfidfSpacy = tfidf_Spacy.transform(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert back to dataframes\n",
    "X_train_tfidfSpacy = pd.DataFrame(X_train_tfidfSpacy.toarray(), columns=tfidf_Spacy.get_feature_names())\n",
    "X_validation_tfidfSpacy = pd.DataFrame(X_validation_tfidfSpacy.toarray(), columns=tfidf_Spacy.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF NLTK\n",
    "tfidf_NLTK = TfidfVectorizer(min_df=10, tokenizer=tokenizeNLTK, ngram_range = (1,3))\n",
    "tfidf_NLTK.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform with NLTK CountVectorizer\n",
    "X_train_tfidfNLTK = tfidf_NLTK.transform(X_train)\n",
    "X_validation_tfidfNLTK = tfidf_NLTK.transform(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert back to dataframes\n",
    "X_train_tfidfNLTK = pd.DataFrame(X_train_tfidfNLTK.toarray(), columns=tfidf_NLTK.get_feature_names())\n",
    "X_validation_tfidfNLTK = pd.DataFrame(X_validation_tfidfNLTK.toarray(), columns=tfidf_NLTK.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model for TF-IDF & CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BrainStationCapstone",
   "language": "python",
   "name": "brainstationcapstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
