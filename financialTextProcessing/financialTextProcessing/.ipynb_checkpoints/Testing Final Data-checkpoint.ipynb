{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loadArticles\n",
    "import loadStockInfo\n",
    "import textProcessing\n",
    "\n",
    "import en_core_web_lg\n",
    "\n",
    "import requests\n",
    "import urllib.parse\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.pipeline import EntityRecognizer\n",
    "from spacy.tokens import Doc\n",
    "from spacy.pipeline import EntityRuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sklearn packages\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load stock info using meta and historical data\n",
    "stockInfo = loadStockInfo.loadStockInfo('../../data/stock-market-dataset/stocks/', '../../data/stock-market-dataset/symbols_valid_meta.csv', DEBUG=True)\n",
    "#stock_df = stockInfo.loadStockDf()\n",
    "stock_meta_df = pd.read_csv('../../data/stock-market-dataset/symbols_valid_meta.csv', index_col=0)\n",
    "#stock_meta_df = stock_meta_df[['Symbol', 'Security Name']]\n",
    "stock_meta_df['Tag'] = 'ORG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_df = stock_meta_df[stock_meta_df['ETF'] == 'N']['NASDAQ Symbol']\n",
    "symbol_df = symbol_df[symbol_df != 'FB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_by_ticker_dict = {'ticker': [], 'title': [], 'sentiment': [], 'date': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          A\n",
       "1         AA\n",
       "2       AACG\n",
       "3        AAL\n",
       "4       AAMC\n",
       "        ... \n",
       "5878     ZUO\n",
       "5879     ZVO\n",
       "5880    ZYME\n",
       "5881    ZYNE\n",
       "5882    ZYXI\n",
       "Name: NASDAQ Symbol, Length: 5883, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol_df[symbol_df == 'UTX#']\n",
    "# symbol_df.iloc[54] = 'CARR'\n",
    "symbol_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies').content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html(html_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_500 = tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     MMM\n",
       "1     ABT\n",
       "2    ABBV\n",
       "3    ABMD\n",
       "4     ACN\n",
       "Name: Symbol, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list of symbols to build news from focus on S&P 500 Index Stocks\n",
    "SP_500['Symbol'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "req = requests.get(url=\"https://stocknewsapi.com/api/v1?tickers=MMM&items=50&token=udkw09gzwcqp3zffbhsrcgiqegnytwyda7hlxazy&page=12\")\n",
    "response = req.json()\n",
    "print(response['error'] is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMM - 1/11\n",
      "MMM - 2/11\n",
      "MMM - 3/11\n",
      "MMM - 4/11\n",
      "MMM - 5/11\n",
      "MMM - 6/11\n",
      "MMM - 7/11\n",
      "MMM - 8/11\n",
      "MMM - 9/11\n",
      "MMM - 10/11\n",
      "MMM - 11/11\n",
      "ABBV - 11/13\n",
      "ABBV - 12/13\n",
      "ABBV - 13/13\n",
      "AMD - 13/25\n",
      "AMD - 14/25\n",
      "AMD - 15/25\n",
      "AMD - 16/25\n",
      "AMD - 17/25\n",
      "AMD - 18/25\n",
      "AMD - 19/25\n",
      "AMD - 20/25\n",
      "AMD - 21/25\n",
      "AMD - 22/25\n",
      "AMD - 23/25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-f15c41fb6d20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mstock_news_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://stocknewsapi.com/api/v1?tickers={}&items=50&token={}&page={}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstock_news_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/BrainStationCapstone/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/BrainStationCapstone/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/BrainStationCapstone/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m         }\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/BrainStationCapstone/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/BrainStationCapstone/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/BrainStationCapstone/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m             )\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/BrainStationCapstone/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/BrainStationCapstone/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/BrainStationCapstone/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/BrainStationCapstone/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             )\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/BrainStationCapstone/lib/python3.6/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#max 50 per request\n",
    "token_api = \"udkw09gzwcqp3zffbhsrcgiqegnytwyda7hlxazy\"\n",
    "#f\"https://stocknewsapi.com/api/v1?tickers={ticker}&items=10000&token={token_api}\"\n",
    "\n",
    "for ticker in SP_500['Symbol']:\n",
    "    #number of pages\n",
    "    number_of_pages = 1\n",
    "    num = number_of_pages\n",
    "    while num <= number_of_pages:\n",
    "        # for ticker in symbol_df:\n",
    "        stock_news_url = str(\"https://stocknewsapi.com/api/v1?tickers={}&items=50&token={}&page={}\").format(ticker, token_api, num)\n",
    "\n",
    "        req = requests.get(url=stock_news_url)\n",
    "        response = req.json()\n",
    "        \n",
    "        if 'error' in response:\n",
    "            break\n",
    "        else:\n",
    "            number_of_pages = int(response['total_pages'])\n",
    "            \n",
    "        if response['data'] is None:\n",
    "            continue\n",
    "        else:\n",
    "            for i in response['data']:\n",
    "                news_by_ticker_dict['ticker'].append(ticker)\n",
    "                news_by_ticker_dict['title'].append(i['title'])\n",
    "                news_by_ticker_dict['sentiment'].append(i['sentiment'])\n",
    "                news_by_ticker_dict['date'].append(i['date'])\n",
    "        print(f\"{ticker} - {num}/{number_of_pages}\", end='\\n')\n",
    "        num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Stock Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Feature Price Change of Day (Positive/Negative/Neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZYXIW\r"
     ]
    }
   ],
   "source": [
    "stock_df = stockInfo.loadStockDf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24197442, 9)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = stock_df.query('Date > \"2018-01-30\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2986292, 9)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df.columns = ['date', 'open', 'high', 'low', 'close', 'adj_close', 'volume', 'symbol','name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>symbol</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4578</th>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>73.769997</td>\n",
       "      <td>74.389999</td>\n",
       "      <td>73.239998</td>\n",
       "      <td>73.430000</td>\n",
       "      <td>71.956703</td>\n",
       "      <td>2032800.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies, Inc. Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4579</th>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>73.180000</td>\n",
       "      <td>73.779999</td>\n",
       "      <td>72.510002</td>\n",
       "      <td>72.830002</td>\n",
       "      <td>71.368736</td>\n",
       "      <td>2008200.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies, Inc. Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4580</th>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>72.320000</td>\n",
       "      <td>72.760002</td>\n",
       "      <td>71.220001</td>\n",
       "      <td>71.250000</td>\n",
       "      <td>69.820442</td>\n",
       "      <td>1955700.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies, Inc. Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4581</th>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>70.860001</td>\n",
       "      <td>71.480003</td>\n",
       "      <td>68.180000</td>\n",
       "      <td>68.220001</td>\n",
       "      <td>66.851227</td>\n",
       "      <td>2860700.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies, Inc. Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4582</th>\n",
       "      <td>2018-02-06</td>\n",
       "      <td>66.959999</td>\n",
       "      <td>68.830002</td>\n",
       "      <td>66.129997</td>\n",
       "      <td>68.449997</td>\n",
       "      <td>67.076614</td>\n",
       "      <td>4121200.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies, Inc. Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24197437</th>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>10.230000</td>\n",
       "      <td>11.430000</td>\n",
       "      <td>10.230000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>189500.0</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>Zynex, Inc. - Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24197438</th>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>10.980000</td>\n",
       "      <td>10.060000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>145000.0</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>Zynex, Inc. - Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24197439</th>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>10.160000</td>\n",
       "      <td>11.060000</td>\n",
       "      <td>10.160000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>162300.0</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>Zynex, Inc. - Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24197440</th>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>10.680000</td>\n",
       "      <td>11.140000</td>\n",
       "      <td>10.590000</td>\n",
       "      <td>11.070000</td>\n",
       "      <td>11.070000</td>\n",
       "      <td>280400.0</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>Zynex, Inc. - Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24197441</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>11.160000</td>\n",
       "      <td>11.160000</td>\n",
       "      <td>10.510000</td>\n",
       "      <td>10.920000</td>\n",
       "      <td>10.920000</td>\n",
       "      <td>315900.0</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>Zynex, Inc. - Common Stock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2986292 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                date       open       high        low      close  adj_close  \\\n",
       "4578      2018-01-31  73.769997  74.389999  73.239998  73.430000  71.956703   \n",
       "4579      2018-02-01  73.180000  73.779999  72.510002  72.830002  71.368736   \n",
       "4580      2018-02-02  72.320000  72.760002  71.220001  71.250000  69.820442   \n",
       "4581      2018-02-05  70.860001  71.480003  68.180000  68.220001  66.851227   \n",
       "4582      2018-02-06  66.959999  68.830002  66.129997  68.449997  67.076614   \n",
       "...              ...        ...        ...        ...        ...        ...   \n",
       "24197437  2020-03-26  10.230000  11.430000  10.230000  11.100000  11.100000   \n",
       "24197438  2020-03-27  10.700000  10.980000  10.060000  10.300000  10.300000   \n",
       "24197439  2020-03-30  10.160000  11.060000  10.160000  10.800000  10.800000   \n",
       "24197440  2020-03-31  10.680000  11.140000  10.590000  11.070000  11.070000   \n",
       "24197441  2020-04-01  11.160000  11.160000  10.510000  10.920000  10.920000   \n",
       "\n",
       "             volume symbol                                     name  \n",
       "4578      2032800.0      A  Agilent Technologies, Inc. Common Stock  \n",
       "4579      2008200.0      A  Agilent Technologies, Inc. Common Stock  \n",
       "4580      1955700.0      A  Agilent Technologies, Inc. Common Stock  \n",
       "4581      2860700.0      A  Agilent Technologies, Inc. Common Stock  \n",
       "4582      4121200.0      A  Agilent Technologies, Inc. Common Stock  \n",
       "...             ...    ...                                      ...  \n",
       "24197437   189500.0   ZYXI               Zynex, Inc. - Common Stock  \n",
       "24197438   145000.0   ZYXI               Zynex, Inc. - Common Stock  \n",
       "24197439   162300.0   ZYXI               Zynex, Inc. - Common Stock  \n",
       "24197440   280400.0   ZYXI               Zynex, Inc. - Common Stock  \n",
       "24197441   315900.0   ZYXI               Zynex, Inc. - Common Stock  \n",
       "\n",
       "[2986292 rows x 9 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AA</th>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AACG</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAL</th>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAMC</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZUO</th>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZVO</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZYME</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZYNE</th>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZYXI</th>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5034 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        title  sentiment  date\n",
       "ticker                        \n",
       "A          50         50    50\n",
       "AA         50         50    50\n",
       "AACG        5          5     5\n",
       "AAL        50         50    50\n",
       "AAMC        4          4     4\n",
       "...       ...        ...   ...\n",
       "ZUO        50         50    50\n",
       "ZVO        15         15    15\n",
       "ZYME       16         16    16\n",
       "ZYNE       50         50    50\n",
       "ZYXI       35         35    35\n",
       "\n",
       "[5034 rows x 3 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_by_ticker_df.groupby(['ticker']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_by_ticker_df = pd.read_csv(\"../../data/news/news_by_ticker.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    59485\n",
       "2    55150\n",
       "1    19976\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_by_ticker_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.bar(news_by_ticker_df['sentiment'].value_counts().index, news_by_ticker_df['sentiment'].value_counts().values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134611, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_by_ticker_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get entities from article headline\n",
    "nlp = en_core_web_lg.load()\n",
    "ruler = EntityRuler(nlp)\n",
    "\n",
    "#creating pattersn for entity ruler\n",
    "temp_df1 = stock_meta_df[['Tag', 'Symbol']]\n",
    "temp_df1.columns = ['label', 'pattern']\n",
    "temp_df2 = stock_meta_df[['Tag', 'Name']]\n",
    "temp_df2.columns = ['label', 'pattern']\n",
    "\n",
    "temp_df = pd.concat([temp_df1, temp_df2])\n",
    "\n",
    "#set patterns\n",
    "patterns = temp_df.to_dict('records')\n",
    "#add patterns to ruler\n",
    "ruler.add_patterns(patterns)\n",
    "#add ruler to nlp\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "tp = textProcessing.textProcessing(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format date to connect stock info later\n",
    "news_by_ticker_df['date'] = pd.to_datetime(pd.to_datetime(news_by_ticker_df['date'], utc=True).dt.strftime(\"%m/%d/%y\"))\n",
    "\n",
    "#now that we have news_df which has the stock symbol we can derive sentiment and compare with historical price changes\n",
    "#create mutliple sets of transformed text, train test split data X is Headline y is sentiment\n",
    "#convert sentiment\n",
    "# 0 -> Positive\n",
    "# 1 -> Negative\n",
    "# 2 -> Neutral\n",
    "news_by_ticker_df['sentiment'] = np.where(news_by_ticker_df['sentiment'] == 'Positive', 0, news_by_ticker_df['sentiment'])\n",
    "news_by_ticker_df['sentiment'] = np.where(news_by_ticker_df['sentiment'] == 'Negative', 1, news_by_ticker_df['sentiment'])\n",
    "news_by_ticker_df['sentiment'] = np.where(news_by_ticker_df['sentiment'] == 'Neutral', 2, news_by_ticker_df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticker               object\n",
      "title                object\n",
      "sentiment             int32\n",
      "date         datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "news_by_ticker_df = news_by_ticker_df.astype({'sentiment': 'int32'})\n",
    "print(news_by_ticker_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#set X and y\n",
    "X = news_by_ticker_df['title']\n",
    "y = news_by_ticker_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since the size of the df is (134611, 4) I feel we can have a validation set\n",
    "X_remainder, X_test, y_remainder, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_remainder, y_remainder, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75381,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32307,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text processing\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " def tokenizeNLTK(sentence):\n",
    "    try:\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        ENGLISH_STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "        for punctuation_mark in string.punctuation:\n",
    "            # Remove punctuation and set to lower case\n",
    "            sentence = sentence.replace(punctuation_mark,'').lower()\n",
    "\n",
    "        # split sentence into words\n",
    "        listofwords = sentence.split(' ')\n",
    "        listofstemmed_words = []\n",
    "\n",
    "\n",
    "        # Remove stopwords and any tokens that are just empty strings\n",
    "        for word in listofwords:\n",
    "            if (not word in ENGLISH_STOP_WORDS) and (word!=''):\n",
    "                # Stem words\n",
    "                stemmed_word = stemmer.stem(word)\n",
    "                listofstemmed_words.append(stemmed_word)\n",
    "\n",
    "        return listofstemmed_words\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Something went wrong in tokenizeNLTK: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeSpacy(sentence):\n",
    "    try:\n",
    "        for punctuation_mark in string.punctuation:\n",
    "            # Remove punctuation and set to lower case\n",
    "            sentence = sentence.replace(punctuation_mark,'').lower()\n",
    "\n",
    "        doc = nlp(sentence)\n",
    "\n",
    "        listofwords = list()\n",
    "        for token in doc:\n",
    "            if not token.is_stop:\n",
    "                if token.is_alpha:\n",
    "                    listofwords.append(token.lemma_.strip().lower())\n",
    "\n",
    "        return listofwords\n",
    "    except Exception as e:\n",
    "        print(f\"Something went wrong in tokenizeSpacy: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_line = \"\\n=======================================================\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer on X_train, X_validation using Spacy\n",
    "countVectorizerSpacy = CountVectorizer(tokenizer=tokenizeSpacy, min_df=10, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectorizerSpacy.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform with Spacy CountVectorizer\n",
    "X_train_countSpacy = countVectorizerSpacy.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countSpacy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation_countSpacy = countVectorizerSpacy.transform(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert back to dataframes\n",
    "X_train_countSpacy = pd.DataFrame(X_train_countSpacy.toarray(), columns=countVectorizerSpacy.get_feature_names())\n",
    "X_validation_countSpacy = pd.DataFrame(X_validation_countSpacy.toarray(), columns=countVectorizerSpacy.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train shape: {X_train_countSpacy.shape} - X_validation shape: {X_validation_countSpacy.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try different models see accuracy\n",
    "#for CountVectorize we try LogisticRegression & Decision Trees\n",
    "#train and run on validation set to hypertune Spacy\n",
    "logit_spacy = LogisticRegression(max_iter=1000)\n",
    "logit_spacy.fit(X_train_countSpacy, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted\n",
    "y_pred_spacy = logit_spacy.predict(X_validation_countSpacy)\n",
    "report_spacy = classification_report(y_validation, y_pred_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation Spacy Confusion Matrix:\", print_line, confusion_matrix(y_validation, y_pred_spacy))\n",
    "print(\"\\nValidation Spacy Data Classification Report:\", print_line, report_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_range = [.000001, .00001, .0001, .001, .01,.1, 1, 10, 100]\n",
    "train_scores = list()\n",
    "validation_scores = list()\n",
    "for index, c in enumerate(C_range):\n",
    "    print(index, end='\\r')\n",
    "    logit_test = LogisticRegression(C=c, max_iter=1000)\n",
    "    logit_test.fit(X_train_countSpacy, y_train)\n",
    "    validation_scores.append(logit_test.score(X_validation_countSpacy, y_validation))\n",
    "    train_scores.append(logit_test.score(X_train_countSpacy, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot validation and train scores\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.plot(range(6), validation_scores, label=\"Validation\")\n",
    "plt.plot(range(6), train_scores, label=\"Train\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Position for C_range\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_c = C_range[validation_scores.index(np.max(validation_scores))]\n",
    "best_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count vectorizer\n",
    "countVectorizerSpacy.fit(X_remainder)\n",
    "X_remainder_countSpacy = countVectorizerSpacy.transform(X_remainder)\n",
    "X_test_countSpacy = countVectorizerSpacy.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_remainder_countSpacy = pd.DataFrame(X_remainder_countSpacy.toarray(), columns=countVectorizerSpacy.get_feature_names())\n",
    "X_test_countSpacy = pd.DataFrame(X_test_countSpacy.toarray(), columns=countVectorizerSpacy.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_remainder_countSpacy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_countSpacy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run on test set\n",
    "logit_spacy = LogisticRegression(C=best_c, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_spacy.fit(X_remainder_countSpacy, y_remainder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted\n",
    "y_pred_test_spacy = logit_spacy.predict(X_test_countSpacy)\n",
    "report_test_spacy = classification_report(y_test, y_pred_test_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Spacy Confusion Matrix:\", print_line, confusion_matrix(y_test, y_pred_test_spacy))\n",
    "print(\"\\Test Spacy Data Classification Report:\", print_line, report_test_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jjackson/opt/anaconda3/envs/BrainStationCapstone/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(min_df=10, ngram_range=(1, 3),\n",
       "                tokenizer=<function tokenizeNLTK at 0x7fcf62f1ae18>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CountVectorizer on X_train, X_validation using NLTK\n",
    "countVectorizerNLTK = CountVectorizer(tokenizer=tokenizeNLTK, min_df=10, ngram_range=(1,3))\n",
    "countVectorizerNLTK.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform with NLTK CountVectorizer\n",
    "X_train_countNLTK = countVectorizerNLTK.transform(X_train)\n",
    "X_validation_countNLTK = countVectorizerNLTK.transform(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'toarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-808559146fee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#convert back to dataframes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train_countNLTK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_countNLTK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcountVectorizerNLTK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_validation_countNLTK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_validation_countNLTK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcountVectorizerNLTK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/BrainStationCapstone/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'toarray'"
     ]
    }
   ],
   "source": [
    "#convert back to dataframes\n",
    "X_train_countNLTK = pd.DataFrame(X_train_countNLTK.toarray(), columns=countVectorizerNLTK.get_feature_names())\n",
    "X_validation_countNLTK = pd.DataFrame(X_validation_countNLTK.toarray(), columns=countVectorizerNLTK.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (75381, 14866) - X_validation shape: (32307, 14866)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train shape: {X_train_countNLTK.shape} - X_validation shape: {X_validation_countNLTK.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train and run on validation set to hypertune NLTK\n",
    "logit_NLTK = LogisticRegression(max_iter=1000)\n",
    "logit_NLTK.fit(X_train_countNLTK, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_NLTK = logit_NLTK.predict(X_validation_countNLTK)\n",
    "report_NLTK = classification_report(y_validation, y_pred_NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Validation Confusion Matrix: \n",
      "=======================================================\n",
      " [[12762   218  1307]\n",
      " [  369  3915   484]\n",
      " [ 1445   382 11425]]\n",
      "\n",
      "NLTK Validation Data Classification Report: \n",
      "=======================================================\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.88     14287\n",
      "           1       0.87      0.82      0.84      4768\n",
      "           2       0.86      0.86      0.86     13252\n",
      "\n",
      "    accuracy                           0.87     32307\n",
      "   macro avg       0.87      0.86      0.86     32307\n",
      "weighted avg       0.87      0.87      0.87     32307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"NLTK Validation Confusion Matrix:\", print_line, confusion_matrix(y_validation, y_pred_NLTK))\n",
    "print(\"\\nNLTK Validation Data Classification Report:\", print_line, report_NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count vectorizer\n",
    "countVectorizerNLTK.fit(X_remainder)\n",
    "X_remainder_countNLTK = countVectorizerNLTK.transform(X_remainder)\n",
    "X_test_countNLTK = countVectorizerNLTK.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_remainder_countNLTK = pd.DataFrame(X_remainder_countNLTK.toarray(), columns=countVectorizerNLTK.get_feature_names())\n",
    "X_test_countNLTK = pd.DataFrame(X_test_countNLTK.toarray(), columns=countVectorizerNLTK.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run on test set\n",
    "logit_NLTK = LogisticRegression(C=0.01, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, max_iter=1000)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_NLTK.fit(X_remainder_countNLTK, y_remainder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted\n",
    "y_pred_test_nltk = logit_NLTK.predict(X_test_countNLTK)\n",
    "report_test_nltk = classification_report(y_test, y_pred_test_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Spacy Confusion Matrix: \n",
      "=======================================================\n",
      " [[10494    98  1245]\n",
      " [  641  2643   746]\n",
      " [ 1378   146  9532]]\n",
      "\n",
      "Test Spacy Data Classification Report: \n",
      "=======================================================\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86     11837\n",
      "           1       0.92      0.66      0.76      4030\n",
      "           2       0.83      0.86      0.84     11056\n",
      "\n",
      "    accuracy                           0.84     26923\n",
      "   macro avg       0.86      0.80      0.82     26923\n",
      "weighted avg       0.85      0.84      0.84     26923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Spacy Confusion Matrix:\", print_line, confusion_matrix(y_test, y_pred_test_nltk))\n",
    "print(\"\\nTest Spacy Data Classification Report:\", print_line, report_test_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Spacy\n",
    "tfidf_Spacy = TfidfVectorizer(min_df=10, tokenizer=tokenizeSpacy, ngram_range = (1,3))\n",
    "tfidf_Spacy.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform with NLTK CountVectorizer\n",
    "X_train_tfidfSpacy = tfidf_Spacy.transform(X_train)\n",
    "X_validation_tfidfSpacy = tfidf_Spacy.transform(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert back to dataframes\n",
    "X_train_tfidfSpacy = pd.DataFrame(X_train_tfidfSpacy.toarray(), columns=tfidf_Spacy.get_feature_names())\n",
    "X_validation_tfidfSpacy = pd.DataFrame(X_validation_tfidfSpacy.toarray(), columns=tfidf_Spacy.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF NLTK\n",
    "tfidf_NLTK = TfidfVectorizer(min_df=10, tokenizer=tokenizeNLTK, ngram_range = (1,3))\n",
    "tfidf_NLTK.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform with NLTK CountVectorizer\n",
    "X_train_tfidfNLTK = tfidf_NLTK.transform(X_train)\n",
    "X_validation_tfidfNLTK = tfidf_NLTK.transform(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert back to dataframes\n",
    "X_train_tfidfNLTK = pd.DataFrame(X_train_tfidfNLTK.toarray(), columns=tfidf_NLTK.get_feature_names())\n",
    "X_validation_tfidfNLTK = pd.DataFrame(X_validation_tfidfNLTK.toarray(), columns=tfidf_NLTK.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model for TF-IDF & CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BrainStationCapstone",
   "language": "python",
   "name": "brainstationcapstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
